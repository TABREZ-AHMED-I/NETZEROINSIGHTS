{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnHcYRmB+L57Z1tGVnBqhZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TABREZ-AHMED-I/NETZEROINSIGHTS/blob/main/netzeroinsights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qyBoluvIDSum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "import os\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "class CompanyScraper:\n",
        "    def __init__(self, company_name, website_url):\n",
        "        self.company_name = company_name\n",
        "        self.base_url = website_url\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        })\n",
        "\n",
        "    def fetch_page(self, url):\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            return response\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def extract_company_description(self, soup):\n",
        "        description = \"\"\n",
        "        meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
        "        if meta_desc:\n",
        "            description = meta_desc.get('content', '').strip()\n",
        "\n",
        "        if not description:\n",
        "            og_desc = soup.find('meta', attrs={'property': 'og:description'})\n",
        "            if og_desc:\n",
        "                description = og_desc.get('content', '').strip()\n",
        "\n",
        "        if not description:\n",
        "            hero_sections = soup.find_all(['div', 'section'],\n",
        "                                        class_=re.compile(r'hero|banner|about|intro', re.IGNORECASE))\n",
        "            for section in hero_sections:\n",
        "                paragraphs = section.find_all('p')\n",
        "                for p in paragraphs:\n",
        "                    text = p.get_text(strip=True)\n",
        "                    if len(text) > 50:\n",
        "                        description = text\n",
        "                        break\n",
        "                if description:\n",
        "                    break\n",
        "\n",
        "        if not description:\n",
        "            paragraphs = soup.find_all('p')\n",
        "            for p in paragraphs:\n",
        "                text = p.get_text(strip=True)\n",
        "                if len(text) > 100:\n",
        "                    description = text\n",
        "                    break\n",
        "\n",
        "        return description if description else \"Description not available\"\n",
        "\n",
        "    def extract_office_locations(self, soup):\n",
        "        hq_location = \"\"\n",
        "        offices = []\n",
        "\n",
        "        contact_sections = soup.find_all(['div', 'section'],\n",
        "                                       class_=re.compile(r'contact|location|address|office|footer', re.IGNORECASE))\n",
        "\n",
        "        for section in contact_sections:\n",
        "            text_content = section.get_text()\n",
        "            addresses = re.findall(r'[A-Za-z0-9\\s,]+(?:Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Drive|Dr|Court|Ct|Place|Pl|Lane|Ln)[A-Za-z0-9\\s,]*', text_content)\n",
        "            if addresses:\n",
        "                offices.extend(addresses)\n",
        "\n",
        "        location_text = soup.find_all(string=re.compile(r'location|address|office|headquarters', re.IGNORECASE))\n",
        "        for text in location_text:\n",
        "            parent = text.parent\n",
        "            if parent:\n",
        "                location_content = parent.get_text(strip=True)\n",
        "                if len(location_content) > 10:\n",
        "                    offices.append(location_content)\n",
        "\n",
        "        offices = list(set(offices))\n",
        "        offices = [office for office in offices if len(office) > 5]\n",
        "\n",
        "        if not offices:\n",
        "            contact_links = soup.find_all('a', href=re.compile(r'contact|about|location', re.IGNORECASE))\n",
        "            contact_urls = []\n",
        "\n",
        "            for link in contact_links:\n",
        "                contact_url = urljoin(self.base_url, link.get('href', ''))\n",
        "                if contact_url not in contact_urls:\n",
        "                    contact_urls.append(contact_url)\n",
        "\n",
        "            if contact_urls:\n",
        "                contact_url = contact_urls[0]\n",
        "                offices.append(f\"Contact: {contact_url}\")\n",
        "                hq_location = f\"Contact: {contact_url}\"\n",
        "            else:\n",
        "                contact_url = f\"{self.base_url}/contact\"\n",
        "                offices.append(f\"Contact: {contact_url}\")\n",
        "                hq_location = f\"Contact: {contact_url}\"\n",
        "        else:\n",
        "            hq_location = offices[0]\n",
        "\n",
        "        return hq_location, offices\n",
        "\n",
        "    def extract_clients(self, soup):\n",
        "        clients = []\n",
        "\n",
        "        all_images = soup.find_all('img')\n",
        "        for img in all_images:\n",
        "            alt_text = img.get('alt', '')\n",
        "            src = img.get('src', '')\n",
        "\n",
        "            if not alt_text or any(keyword in alt_text.lower() for keyword in\n",
        "                                 ['logo', 'icon', 'image', 'picture', 'avatar', 'button', 'menu', 'header', 'footer']):\n",
        "                continue\n",
        "\n",
        "            if src and any(ext in src.lower() for ext in ['.png', '.jpg', '.jpeg', '.svg', '.webp']):\n",
        "                parent = img.find_parent(['div', 'section'])\n",
        "                if parent:\n",
        "                    parent_text = parent.get_text().lower()\n",
        "                    if any(keyword in parent_text for keyword in\n",
        "                          ['client', 'customer', 'partner', 'customer', 'work with', 'trusted by', 'featured']):\n",
        "                        full_image_url = urljoin(self.base_url, src)\n",
        "                        client_data = {\n",
        "                            'name': alt_text.title(),\n",
        "                            'image_url': full_image_url\n",
        "                        }\n",
        "                        if not any(c['name'] == client_data['name'] for c in clients):\n",
        "                            clients.append(client_data)\n",
        "\n",
        "        client_headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'],\n",
        "                                     string=re.compile(r'client|customer|partner|work with|trusted by|featured', re.IGNORECASE))\n",
        "\n",
        "        for header in client_headers:\n",
        "            container = header.find_parent(['div', 'section']) or header.find_next_sibling(['div', 'section'])\n",
        "            if container:\n",
        "                images = container.find_all('img')\n",
        "                for img in images:\n",
        "                    alt_text = img.get('alt', '')\n",
        "                    src = img.get('src', '')\n",
        "                    if alt_text and src:\n",
        "                        full_image_url = urljoin(self.base_url, src)\n",
        "                        client_data = {\n",
        "                            'name': alt_text.title(),\n",
        "                            'image_url': full_image_url\n",
        "                        }\n",
        "                        if not any(c['name'] == client_data['name'] for c in clients):\n",
        "                            clients.append(client_data)\n",
        "\n",
        "        logo_sections = soup.find_all(['div', 'section'],\n",
        "                                    class_=re.compile(r'logo|client|customer|partner|brand|grid|carousel|slider', re.IGNORECASE))\n",
        "\n",
        "        for section in logo_sections:\n",
        "            images = section.find_all('img')\n",
        "            for img in images:\n",
        "                alt_text = img.get('alt', '')\n",
        "                src = img.get('src', '')\n",
        "                if alt_text and src:\n",
        "                    if any(ext in src.lower() for ext in ['.png', '.jpg', '.jpeg', '.svg', '.webp']):\n",
        "                        full_image_url = urljoin(self.base_url, src)\n",
        "                        client_data = {\n",
        "                            'name': alt_text.title() if alt_text else \"Unnamed Client\",\n",
        "                            'image_url': full_image_url\n",
        "                        }\n",
        "                        if not any(c['name'] == client_data['name'] for c in clients):\n",
        "                            clients.append(client_data)\n",
        "\n",
        "        if not clients:\n",
        "            client_names = []\n",
        "            client_text_sections = soup.find_all(string=re.compile(r'client|customer|partner', re.IGNORECASE))\n",
        "            for text in client_text_sections:\n",
        "                parent = text.parent\n",
        "                if parent:\n",
        "                    siblings = parent.find_next_siblings(['li', 'div', 'span'])\n",
        "                    for sibling in siblings[:5]:\n",
        "                        client_text = sibling.get_text(strip=True)\n",
        "                        if (client_text and len(client_text) > 2 and len(client_text) < 100 and\n",
        "                            not any(keyword in client_text.lower() for keyword in ['client', 'customer', 'partner'])):\n",
        "                            if client_text not in client_names:\n",
        "                                client_names.append(client_text)\n",
        "                                clients.append({\n",
        "                                    'name': client_text,\n",
        "                                    'image_url': 'No image available'\n",
        "                                })\n",
        "\n",
        "        return clients[:20]\n",
        "\n",
        "    def extract_news_articles(self, soup):\n",
        "        news_items = []\n",
        "\n",
        "        news_selectors = [\n",
        "            'article',\n",
        "            '[class*=\"news\"]',\n",
        "            '[class*=\"blog\"]',\n",
        "            '[class*=\"post\"]',\n",
        "            '[class*=\"article\"]',\n",
        "            '[class*=\"update\"]',\n",
        "            '[class*=\"press\"]',\n",
        "            '[class*=\"media\"]',\n",
        "            '[class*=\"story\"]',\n",
        "            '[class*=\"feature\"]'\n",
        "        ]\n",
        "\n",
        "        for selector in news_selectors:\n",
        "            elements = soup.select(selector)\n",
        "            for element in elements[:10]:\n",
        "                try:\n",
        "                    title = None\n",
        "                    title_elements = element.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
        "                    for title_elem in title_elements:\n",
        "                        if title_elem.get_text(strip=True):\n",
        "                            title = title_elem.get_text(strip=True)\n",
        "                            break\n",
        "\n",
        "                    if not title:\n",
        "                        title_link = element.find('a')\n",
        "                        if title_link and title_link.get_text(strip=True):\n",
        "                            title = title_link.get_text(strip=True)\n",
        "\n",
        "                    if not title or len(title) < 5:\n",
        "                        continue\n",
        "\n",
        "                    date = \"Recent\"\n",
        "                    date_elements = element.find_all(['time', '[class*=\"date\"]', '[class*=\"time\"]'])\n",
        "                    for date_elem in date_elements:\n",
        "                        if date_elem.get_text(strip=True):\n",
        "                            date = date_elem.get_text(strip=True)\n",
        "                            break\n",
        "\n",
        "                    url = f\"{self.base_url}/news\"\n",
        "                    link_elem = element.find('a')\n",
        "                    if link_elem and link_elem.get('href'):\n",
        "                        url = urljoin(self.base_url, link_elem['href'])\n",
        "\n",
        "                    summary = \"\"\n",
        "                    summary_elements = element.find_all(['p', '[class*=\"summary\"]', '[class*=\"excerpt\"]', '[class*=\"description\"]'])\n",
        "                    for summary_elem in summary_elements:\n",
        "                        text = summary_elem.get_text(strip=True)\n",
        "                        if text and len(text) > 20:\n",
        "                            summary = text[:300]\n",
        "                            break\n",
        "\n",
        "                    if not summary:\n",
        "                        first_p = element.find('p')\n",
        "                        if first_p:\n",
        "                            summary = first_p.get_text(strip=True)[:300]\n",
        "                        else:\n",
        "                            summary = f\"Read more about {title}\"\n",
        "\n",
        "                    news_item = {\n",
        "                        'title': title,\n",
        "                        'date': date,\n",
        "                        'url': url,\n",
        "                        'summary': summary\n",
        "                    }\n",
        "\n",
        "                    is_duplicate = False\n",
        "                    for existing in news_items:\n",
        "                        if existing['title'] == title:\n",
        "                            is_duplicate = True\n",
        "                            break\n",
        "\n",
        "                    if not is_duplicate:\n",
        "                        news_items.append(news_item)\n",
        "\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        if not news_items:\n",
        "            content_sections = soup.find_all(['div', 'section'],\n",
        "                                           class_=re.compile(r'content|main|body', re.IGNORECASE))\n",
        "\n",
        "            for section in content_sections[:5]:\n",
        "                headers = section.find_all(['h1', 'h2', 'h3'])\n",
        "                for header in headers:\n",
        "                    title = header.get_text(strip=True)\n",
        "                    if title and len(title) > 5:\n",
        "                        next_p = header.find_next('p')\n",
        "                        summary = next_p.get_text(strip=True)[:200] if next_p else f\"Content about {title}\"\n",
        "\n",
        "                        news_items.append({\n",
        "                            'title': title,\n",
        "                            'date': 'Recent',\n",
        "                            'url': self.base_url,\n",
        "                            'summary': summary\n",
        "                        })\n",
        "                        break\n",
        "\n",
        "                if news_items:\n",
        "                    break\n",
        "\n",
        "        if not news_items:\n",
        "            news_items.append({\n",
        "                'title': f'Latest from {self.company_name}',\n",
        "                'date': '2024',\n",
        "                'url': self.base_url,\n",
        "                'summary': f'Visit {self.company_name} website for latest updates and news'\n",
        "            })\n",
        "\n",
        "        return news_items[:10]\n",
        "\n",
        "    def scrape_company_data(self):\n",
        "        print(f\"Processing: {self.company_name}\")\n",
        "\n",
        "        company_data = {\n",
        "            'company_name': self.company_name,\n",
        "            'website': self.base_url,\n",
        "            'description': '',\n",
        "            'headquarters': '',\n",
        "            'offices': [],\n",
        "            'clients': [],\n",
        "            'news': []\n",
        "        }\n",
        "\n",
        "        response = self.fetch_page(self.base_url)\n",
        "        if not response:\n",
        "            print(f\"Failed to access {self.company_name} website\")\n",
        "            return company_data\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        company_data['description'] = self.extract_company_description(soup)\n",
        "        company_data['headquarters'], company_data['offices'] = self.extract_office_locations(soup)\n",
        "        company_data['clients'] = self.extract_clients(soup)\n",
        "        company_data['news'] = self.extract_news_articles(soup)\n",
        "\n",
        "        print(f\"Completed: {self.company_name}\")\n",
        "        return company_data\n",
        "\n",
        "def read_companies_from_excel():\n",
        "    print(\"Upload Excel File with Company List\")\n",
        "    print(\"Your file should have company names and website URLs\")\n",
        "    print()\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded\")\n",
        "        return None\n",
        "\n",
        "    file_name = list(uploaded.keys())[0]\n",
        "    file_content = uploaded[file_name]\n",
        "\n",
        "    print(f\"File uploaded: {file_name}\")\n",
        "\n",
        "    try:\n",
        "        try:\n",
        "            df = pd.read_excel(io.BytesIO(file_content))\n",
        "        except:\n",
        "            try:\n",
        "                df = pd.read_csv(io.BytesIO(file_content))\n",
        "            except:\n",
        "                df = pd.read_csv(io.BytesIO(file_content), encoding='latin-1')\n",
        "\n",
        "        print(f\"Columns: {list(df.columns)}\")\n",
        "        print(f\"Total rows: {len(df)}\")\n",
        "\n",
        "        name_column = None\n",
        "        url_column = None\n",
        "\n",
        "        name_variations = ['company_name', 'company', 'name', 'organization', 'company name', 'business']\n",
        "        url_variations = ['website_url', 'website', 'url', 'web', 'website url', 'link', 'domain']\n",
        "\n",
        "        for col in df.columns:\n",
        "            col_lower = str(col).lower()\n",
        "            if any(variation in col_lower for variation in name_variations) and not name_column:\n",
        "                name_column = col\n",
        "            if any(variation in col_lower for variation in url_variations) and not url_column:\n",
        "                url_column = col\n",
        "\n",
        "        if not name_column and len(df.columns) >= 1:\n",
        "            name_column = df.columns[0]\n",
        "\n",
        "        if not url_column and len(df.columns) >= 2:\n",
        "            url_column = df.columns[1]\n",
        "\n",
        "        if not name_column or not url_column:\n",
        "            print(\"Could not identify required columns\")\n",
        "            return None\n",
        "\n",
        "        companies = []\n",
        "        valid_count = 0\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            company_name = str(row[name_column]).strip()\n",
        "            website_url = str(row[url_column]).strip()\n",
        "\n",
        "            if (company_name and website_url and\n",
        "                company_name != 'nan' and website_url != 'nan' and\n",
        "                len(company_name) > 1 and len(website_url) > 5):\n",
        "\n",
        "                if not website_url.startswith(('http://', 'https://')):\n",
        "                    website_url = 'https://' + website_url\n",
        "\n",
        "                companies.append({\n",
        "                    'name': company_name,\n",
        "                    'url': website_url\n",
        "                })\n",
        "                valid_count += 1\n",
        "\n",
        "        print(f\"Valid companies: {valid_count}\")\n",
        "\n",
        "        print(\"Companies to scrape:\")\n",
        "        for i, company in enumerate(companies[:5], 1):\n",
        "            print(f\"   {i}. {company['name']} -> {company['url']}\")\n",
        "        if len(companies) > 5:\n",
        "            print(f\"   ... and {len(companies) - 5} more\")\n",
        "\n",
        "        return companies\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "def scrape_multiple_companies(companies_list):\n",
        "    all_company_data = []\n",
        "    successful_scrapes = 0\n",
        "\n",
        "    print(f\"Starting batch scraping for {len(companies_list)} companies\")\n",
        "\n",
        "    for index, company in enumerate(companies_list, 1):\n",
        "        print(f\"Processing {index}/{len(companies_list)}: {company['name']}\")\n",
        "\n",
        "        try:\n",
        "            scraper = CompanyScraper(company['name'], company['url'])\n",
        "            company_data = scraper.scrape_company_data()\n",
        "            all_company_data.append(company_data)\n",
        "            successful_scrapes += 1\n",
        "\n",
        "            print(f\"   Description: {company_data['description'][:80]}...\")\n",
        "            print(f\"   HQ: {company_data['headquarters'][:50]}\")\n",
        "            print(f\"   Offices: {len(company_data['offices'])} | Clients: {len(company_data['clients'])} | News: {len(company_data['news'])}\")\n",
        "\n",
        "            time.sleep(2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Failed to scrape {company['name']}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Batch processing completed: {successful_scrapes}/{len(companies_list)} companies\")\n",
        "\n",
        "    return all_company_data\n",
        "\n",
        "def create_consolidated_excel(all_company_data, filename='all_companies_data.xlsx'):\n",
        "    all_companies_df = pd.DataFrame()\n",
        "    all_offices_df = pd.DataFrame()\n",
        "    all_clients_df = pd.DataFrame()\n",
        "    all_news_df = pd.DataFrame()\n",
        "\n",
        "    company_id = 1\n",
        "\n",
        "    for company_data in all_company_data:\n",
        "        company_row = {\n",
        "            'company_id': company_id,\n",
        "            'company_name': company_data['company_name'],\n",
        "            'website': company_data['website'],\n",
        "            'description': company_data['description'],\n",
        "            'headquarters_location': company_data['headquarters']\n",
        "        }\n",
        "        all_companies_df = pd.concat([all_companies_df, pd.DataFrame([company_row])], ignore_index=True)\n",
        "\n",
        "        for office in company_data['offices']:\n",
        "            office_row = {\n",
        "                'office_id': f\"{company_id}-{len(all_offices_df) + 1}\",\n",
        "                'company_id': company_id,\n",
        "                'location': office,\n",
        "                'is_headquarters': 1 if office == company_data['headquarters'] else 0\n",
        "            }\n",
        "            all_offices_df = pd.concat([all_offices_df, pd.DataFrame([office_row])], ignore_index=True)\n",
        "\n",
        "        for client in company_data['clients']:\n",
        "            client_row = {\n",
        "                'client_id': f\"{company_id}-{len(all_clients_df) + 1}\",\n",
        "                'company_id': company_id,\n",
        "                'client_name': client['name'],\n",
        "                'client_image_url': client['image_url']\n",
        "            }\n",
        "            all_clients_df = pd.concat([all_clients_df, pd.DataFrame([client_row])], ignore_index=True)\n",
        "\n",
        "        for news in company_data['news']:\n",
        "            news_row = {\n",
        "                'news_id': f\"{company_id}-{len(all_news_df) + 1}\",\n",
        "                'company_id': company_id,\n",
        "                'news_title': news['title'],\n",
        "                'news_date': news['date'],\n",
        "                'news_url': news['url'],\n",
        "                'news_summary': news['summary']\n",
        "            }\n",
        "            all_news_df = pd.concat([all_news_df, pd.DataFrame([news_row])], ignore_index=True)\n",
        "\n",
        "        company_id += 1\n",
        "\n",
        "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
        "        all_companies_df.to_excel(writer, sheet_name='companies', index=False)\n",
        "        all_offices_df.to_excel(writer, sheet_name='offices', index=False)\n",
        "        all_clients_df.to_excel(writer, sheet_name='clients', index=False)\n",
        "        all_news_df.to_excel(writer, sheet_name='news', index=False)\n",
        "\n",
        "        for sheet_name in writer.sheets:\n",
        "            worksheet = writer.sheets[sheet_name]\n",
        "            for column in worksheet.columns:\n",
        "                max_length = 0\n",
        "                column_letter = column[0].column_letter\n",
        "                for cell in column:\n",
        "                    try:\n",
        "                        if len(str(cell.value)) > max_length:\n",
        "                            max_length = len(str(cell.value))\n",
        "                    except:\n",
        "                        pass\n",
        "                adjusted_width = min(max_length + 2, 50)\n",
        "                worksheet.column_dimensions[column_letter].width = adjusted_width\n",
        "\n",
        "    print(f\"Excel file created: {filename}\")\n",
        "    return filename\n",
        "\n",
        "def download_excel_file(filename):\n",
        "    try:\n",
        "        files.download(filename)\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not download {filename}: {e}\")\n",
        "\n",
        "def main():\n",
        "    print(\"Batch Company Web Scraper\")\n",
        "    print(\"Upload Excel file with company list to begin\")\n",
        "    print()\n",
        "\n",
        "    companies_list = read_companies_from_excel()\n",
        "\n",
        "    if not companies_list:\n",
        "        print(\"No companies to process\")\n",
        "        return\n",
        "\n",
        "    all_company_data = scrape_multiple_companies(companies_list)\n",
        "\n",
        "    if not all_company_data:\n",
        "        print(\"No data extracted\")\n",
        "        return\n",
        "\n",
        "    print(\"Creating consolidated Excel file...\")\n",
        "    excel_filename = create_consolidated_excel(all_company_data)\n",
        "\n",
        "    print(\"Batch scraping completed successfully!\")\n",
        "    print(f\"Output file: {excel_filename}\")\n",
        "    print(f\"Companies processed: {len(all_company_data)}\")\n",
        "\n",
        "    try:\n",
        "        companies_df = pd.read_excel(excel_filename, sheet_name='companies')\n",
        "        offices_df = pd.read_excel(excel_filename, sheet_name='offices')\n",
        "        clients_df = pd.read_excel(excel_filename, sheet_name='clients')\n",
        "        news_df = pd.read_excel(excel_filename, sheet_name='news')\n",
        "\n",
        "        print(\"Final statistics:\")\n",
        "        print(f\"   Companies: {len(companies_df)}\")\n",
        "        print(f\"   Offices: {len(offices_df)}\")\n",
        "        print(f\"   Clients: {len(clients_df)}\")\n",
        "        print(f\"   News Articles: {len(news_df)}\")\n",
        "\n",
        "        file_size = os.path.getsize(excel_filename) / 1024\n",
        "        print(f\"   File size: {file_size:.1f} KB\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load file for statistics: {e}\")\n",
        "\n",
        "    print(\"Downloading results...\")\n",
        "    download_excel_file(excel_filename)\n",
        "\n",
        "    print(\"All done! Check your downloads for the Excel file.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I-pERQ59CNTu",
        "outputId": "7f1ad1bb-d646-4dcf-c92a-ae4153d07520"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Company Web Scraper\n",
            "Upload Excel file with company list to begin\n",
            "\n",
            "Upload Excel File with Company List\n",
            "Your file should have company names and website URLs\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-73f0e977-d325-4f85-b67a-0f3418f3ca46\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-73f0e977-d325-4f85-b67a-0f3418f3ca46\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving company data.csv to company data.csv\n",
            "File uploaded: company data.csv\n",
            "Columns: ['Company_ID', 'Company_name', 'website_url']\n",
            "Total rows: 15\n",
            "Valid companies: 15\n",
            "Companies to scrape:\n",
            "   1. 5875 -> https://www.solarkal.com/\n",
            "   2. 11917 -> https://h2scan.com/\n",
            "   3. 34005 -> https://www.eocharging.com/\n",
            "   4. 65212 -> https://www.prewave.com/\n",
            "   5. 18533 -> https://www.chargepoint.com/\n",
            "   ... and 10 more\n",
            "Starting batch scraping for 15 companies\n",
            "Processing 1/15: 5875\n",
            "Processing: 5875\n",
            "Completed: 5875\n",
            "   Description: SolarKal is the leading commercial solar advisory and procurement marketplace in...\n",
            "   HQ: s StoryResourcesWhy Go Solar\n",
            "   Offices: 3 | Clients: 2 | News: 1\n",
            "Processing 2/15: 11917\n",
            "Processing: 11917\n",
            "Completed: 11917\n",
            "   Description: H2scan’s proven advanced hydrogen sensor technology, based on R&D, engineering a...\n",
            "   HQ:  Corporate Headquarters 27215 Turnberry Lane \n",
            "   Offices: 3 | Clients: 1 | News: 10\n",
            "Processing 3/15: 34005\n",
            "Processing: 34005\n",
            "Completed: 34005\n",
            "   Description: Our commercial EV charging infrastructure solutions for commercial fleets, offer...\n",
            "   HQ: Choose another country or region to see content sp\n",
            "   Offices: 1 | Clients: 20 | News: 1\n",
            "Processing 4/15: 65212\n",
            "Processing: 65212\n",
            "Completed: 65212\n",
            "   Description: Trusted by industry leaders, Prewave’s AI-powered platform delivers cutting-edge...\n",
            "   HQ: N Integrations Action Platform For Suppliers Solut\n",
            "   Offices: 5 | Clients: 7 | News: 2\n",
            "Processing 5/15: 18533\n",
            "Processing: 18533\n",
            "Completed: 18533\n",
            "   Description: ChargePoint is the world's largest network of electric vehicle (EV) charging sta...\n",
            "   HQ: Address your current and future operational needs \n",
            "   Offices: 5 | Clients: 16 | News: 1\n",
            "Processing 6/15: 2805\n",
            "Processing: 2805\n",
            "Completed: 2805\n",
            "   Description: Driverless technology powering vehicle solutions for airports, logistic and indu...\n",
            "   HQ: United States\n",
            "   Offices: 1 | Clients: 3 | News: 5\n",
            "Processing 7/15: 101741\n",
            "Processing: 101741\n",
            "Completed: 101741\n",
            "   Description: Everstream Analytics transforms risk intelligence with AI-driven insights that e...\n",
            "   HQ: \n",
            "Platform\n",
            "Network Mapping\n",
            "Global Monitoring and Al\n",
            "   Offices: 11 | Clients: 5 | News: 10\n",
            "Processing 8/15: 110133\n",
            "Processing: 110133\n",
            "Completed: 110133\n",
            "   Description: Take control of your sustainability and decarbonization goals with our range of ...\n",
            "   HQ: Email address\n",
            "   Offices: 5 | Clients: 4 | News: 4\n",
            "Processing 9/15: 12605\n",
            "Processing: 12605\n",
            "Completed: 12605\n",
            "   Description: Charm Industrial provides high-quality carbon removal through bio-oil sequestrat...\n",
            "   HQ: self.__next_f.push([1,\"1b:[\\\"$\\\",\\\"section\\\",null,\n",
            "   Offices: 4 | Clients: 8 | News: 4\n",
            "Processing 10/15: 105894\n",
            "Processing: 105894\n",
            "Completed: 105894\n",
            "   Description: One Device. All Networks. No Compromise. This is the world’s most robust platfor...\n",
            "   HQ: \n",
            "\n",
            "\n",
            "\n",
            "Stay up to date\n",
            "Sign up to get updates on the \n",
            "   Offices: 1 | Clients: 0 | News: 10\n",
            "Processing 11/15: 400\n",
            "Processing: 400\n",
            "Completed: 400\n",
            "   Description: Reimagining how we power the planet. Energy storage solutions that reduce energy...\n",
            "   HQ: let transitionTrigger = $(\".transition_trigger\");\n",
            "\n",
            "   Offices: 1 | Clients: 12 | News: 1\n",
            "Processing 12/15: 34204\n",
            "Processing: 34204\n",
            "Completed: 34204\n",
            "   Description: We at BioBTX developed a technology to produce BTX from biomass and plastics. We...\n",
            "   HQ: img:is([sizes=auto i],[sizes^=\"auto,\" i]){contain-\n",
            "   Offices: 1 | Clients: 0 | News: 8\n",
            "Processing 13/15: 6134\n",
            "Processing: 6134\n",
            "Completed: 6134\n",
            "   Description: We store and transport hydrogen in a liquid organic carrier. The missing link to...\n",
            "   HQ: Contact: https://hydrogenious.net/who/#contacts\n",
            "   Offices: 1 | Clients: 7 | News: 7\n",
            "Processing 14/15: 12008\n",
            "Processing: 12008\n",
            "Completed: 12008\n",
            "   Description: Iogen carbon-negative fuel production technology turns agriculture waste into cl...\n",
            "   HQ: $(document).ready(function() {$(\"a[href*=#]:not([h\n",
            "   Offices: 1 | Clients: 4 | News: 1\n",
            "Processing 15/15: 6997\n",
            "Processing: 6997\n",
            "Completed: 6997\n",
            "   Description: Let's make textile circularity an everyday reality. Nothing new needs to be grow...\n",
            "   HQ: var mi_version = '9.8.0';\n",
            "\t\t\t\tvar mi_track_user = \n",
            "   Offices: 4 | Clients: 1 | News: 10\n",
            "Batch processing completed: 15/15 companies\n",
            "Creating consolidated Excel file...\n",
            "Excel file created: all_companies_data.xlsx\n",
            "Batch scraping completed successfully!\n",
            "Output file: all_companies_data.xlsx\n",
            "Companies processed: 15\n",
            "Final statistics:\n",
            "   Companies: 15\n",
            "   Offices: 47\n",
            "   Clients: 90\n",
            "   News Articles: 75\n",
            "   File size: 60.4 KB\n",
            "Downloading results...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_15841623-2f03-4ecd-9087-1b16354a5b7d\", \"all_companies_data.xlsx\", 61895)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: all_companies_data.xlsx\n",
            "All done! Check your downloads for the Excel file.\n"
          ]
        }
      ]
    }
  ]
}